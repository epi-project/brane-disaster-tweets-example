import compute;
import utils;

let repo_owner := "marinoandrea";
let repo_name := "disaster-tweets-brane";

// repository dataset locations
let repo_dataset_filepath_test := "data/test.csv";
let repo_dataset_filepath_train := "data/train.csv";

// distributed filesystem dataset locations excluding 'data' prefix
let dfs_dataset_filepath_test := "test.csv";
let dfs_dataset_filepath_train := "train.csv";

//  distributed filesystem vectors locations
let dfs_vectors_filepath_test := "test_vectors.pickle";
let dfs_vectors_filepath_train := "train_vectors.pickle";

// download and store dataset in the distributed filesystem
print("Downloading the dataset files...");
if (download(repo_owner, repo_name, repo_dataset_filepath_train, dfs_dataset_filepath_train) != 0) { 
    // TODO: handle failure
    print("Train dataset download failed.");
}
if (download(repo_owner, repo_name, repo_dataset_filepath_test, dfs_dataset_filepath_test) != 0) { 
    // TODO: handle failure
    print("Test dataset download failed.");
}

// cleaning the dataset text
print("Cleaning the raw tweets text...");
let path_dataset_clean_train := clean(dfs_dataset_filepath_train);
let path_dataset_clean_test := clean(dfs_dataset_filepath_test);

// obtain tokens from raw text
print("Performing tokenization...");
let path_dataset_tokenized_train := tokenize(path_dataset_clean_train);
let path_dataset_tokenized_test := tokenize(path_dataset_clean_test);

// remove english stopwords
print("Removing stopwords...");
let path_dataset_nostopwords_train := remove_stopwords(path_dataset_tokenized_train);
let path_dataset_nostopwords_test := remove_stopwords(path_dataset_tokenized_test);

// create vectorized versions of the datasets
print("Performing feature vectorization...");
let vectors := create_vectors(
    path_dataset_nostopwords_train, 
    path_dataset_nostopwords_test, 
    dfs_vectors_filepath_train,
    dfs_vectors_filepath_test);

// train the classifier
print("Training the model...");
let path_model := train_model(path_dataset_nostopwords_train, dfs_vectors_filepath_train);

// create a submission
print("Classifying tweets...");
let path_submission := create_submission(
    path_dataset_nostopwords_train,
    dfs_vectors_filepath_train, 
    path_model);

// TODO: visualization

print("Done");
